***ONNXRuntime-basic篇（1）***

# 摘要

# 目录

# 几个QA

## 什么是批处理大小？

**批处理大小（Batch Size）**是指在一次前向传播过程中同时处理的样本数量

## 什么是样本？

样本是深度学习模型推理的基本单位，代表一个完整的数据实例，经过模型处理后产生一个对应的输出结果。

- **单个样本**：[1, 3, 224, 224] - 1个样本，3通道，224x224像素
- **批处理样本**：[256, 3, 224, 224] - 256个样本，每个样本3通道，224x224像素

## 什么是学习率？


### 定义

学习率是控制神经网络参数更新步长的超参数，决定了每次迭代中参数变化的幅度。

```python
# 参数更新公式
θ_new = θ_old - α * ∇θ  # α就是学习率
```

其中：
- $\theta$：模型参数
- $\alpha$：学习率
- $\nabla$：梯度（损失函数对参数的偏导数）

### 作用

```python
# 学习率太大
θ_new = θ_old - 0.1 * ∇θ  # 步长太大，可能跳过最优解
# 结果：训练不稳定，可能发散

# 学习率太小  
θ_new = θ_old - 0.0001 * ∇θ  # 步长太小，收敛极慢
# 结果：训练速度慢，可能陷入局部最优
```

### 影响

```
学习率 = 0.01  →  训练稳定，收敛快
学习率 = 0.1   →  可能不稳定，但收敛更快
学习率 = 1.0   →  很可能发散，训练失败
学习率 = 0.001 →  稳定但很慢
```

## 什么是梯度？

梯度是多变量函数的偏导数向量，表示函数在某个点的最陡峭上升方向。

```python
# 对于函数 f(x, y) = x² + y²
# 梯度 = [∂f/∂x, ∂f/∂y] = [2x, 2y]

# 在点 (3, 4) 处
梯度 = [2×3, 2×4] = [6, 8]
# 这意味着在 (3,4) 点，函数沿 x 方向变化率为 6，沿 y 方向变化率为 8
```

### 梯度在神经网络中的作用

在神经网络中，梯度告诉我们如何调整参数来减少损失：

```python
# 损失函数 L(θ) 对参数 θ 的梯度
∇θ = ∂L/∂θ

# 参数更新规则
θ_new = θ_old - α × ∇θ
# α 是学习率，负号表示向梯度相反方向移动（减少损失）
```

### 梯度的计算：链式法则

在深度网络中，梯度通过反向传播计算，使用链式法则：

```python
# 对于网络：输入 x → 隐藏层 h → 输出 y → 损失 L
# 计算 ∂L/∂x：

∂L/∂x = ∂L/∂y × ∂y/∂h × ∂h/∂x

# 这就是链式法则：复合函数的导数 = 各层导数的乘积
```

