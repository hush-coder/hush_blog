# AI编译器-算法篇（2）SA与配置去重

## 摘要

本文介绍了将模拟退火（SA）算法与遗传算法（GA）融合应用于AI编译器混合精度优化。通过GA进行全局搜索，SA进行局部精细优化，两者优势互补。实验结果表明，该混合方法使HPL-AI性能平均提升4.7倍（0.0631→0.2954 Gflops），最大性能提升4.6倍，同时保持100%通过率。文中详细阐述了算法实现流程，包括SA局部搜索函数设计和主循环集成方法，并通过性能对比表和图直观展示了优化效果。

## 目录

- [AI编译器-算法篇（2）SA与配置去重](#ai编译器-算法篇2sa与配置去重)
  - [摘要](#摘要)
  - [目录](#目录)
  - [前言](#前言)
  - [书接上回](#书接上回)
  - [SA](#sa)
    - [什么是SA](#什么是sa)
      - [定义](#定义)
      - [算法流程](#算法流程)
      - [特性](#特性)
    - [SA与GA的融合](#sa与ga的融合)
      - [1. 优势](#1-优势)
      - [2. 融合的基本方法](#2-融合的基本方法)
      - [3. 具体实现流程](#3-具体实现流程)
      - [具体代码实现](#具体代码实现)
      - [预期效果与实际表现](#预期效果与实际表现)
        - [性能对比表](#性能对比表)
        - [性能提升分析](#性能提升分析)
        - [关键指标对比](#关键指标对比)
      - [对比图](#对比图)
        - [性能对比图 (performance\_comparison.png)](#性能对比图-performance_comparisonpng)
        - [优化流程图 (optimization\_flow.png)](#优化流程图-optimization_flowpng)
        - [收敛曲线对比图 (convergence\_comparison.png)](#收敛曲线对比图-convergence_comparisonpng)
        - [参数分析图 (parameter\_analysis.png)](#参数分析图-parameter_analysispng)
  - [配置去重](#配置去重)
    - [什么是哈希值](#什么是哈希值)
      - [定义](#定义-1)
      - [特点](#特点)
      - [常见哈希算法](#常见哈希算法)
    - [什么是配置去重](#什么是配置去重)
      - [核心思想](#核心思想)
      - [优势](#优势)
      - [实现细节](#实现细节)
        - [1. 配置哈希生成](#1-配置哈希生成)
        - [2. 去重存储与检查](#2-去重存储与检查)
        - [3. 缓存持久化](#3-缓存持久化)
        - [4. 种群生成与进化中的去重](#4-种群生成与进化中的去重)
        - [5. 适应度评估中的去重](#5-适应度评估中的去重)
      - [流程图](#流程图)


## 前言

本人正在搞AI编译器，这个博客大家可以当作学习笔记

## 书接上回

正在做混合精度优化的pass，需要对搜出的变量进行离散优化问题的搜索，所以目前第一个想到的是GA搜索算法。

目前已经完成对于GA的基本实现，接下来要实现GA与SA、GA与配置去重的融合

## SA

### 什么是SA

#### 定义

**模拟退火（Simulated Annealing, 简称SA）** 是一种常用的全局优化算法，主要用于在大规模、复杂的搜索空间中寻找最优解。

#### 算法流程

1. **初始化：** 随机生成一个初始解，并设定初始温度T。
2. **扰动：** 在当前解的邻域内随机产生一个新解。
3. **接受准则：** 如果新解比当前解更优（能量更低），则接受新解；如果新解更差，以一定概率接受新解。
4. **降温：** 按照某种规则逐步降低温度T（如T = αT，0 < α < 1）。
5. **终止条件：** 温度降到某个阈值或达到最大迭代次数时停止。

#### 特性

- **跳出局部最优：** 通过以概率接受较差解，SA能有效跳出局部最优
- **参数简单：** 主要参数为初始温度、降温速率和终止温度。
- **适用范围广：** 可用于组合优化、函数优化等多种问题。

### SA与GA的融合

#### 1. 优势

- **遗传算法（GA）** 适合全局搜索，能在大空间内跳出局部最优，但局部精细搜索能力有限，容易在后期收敛速度变慢。
- **模拟退火（SA）** 适合局部搜索，能以概率接受较差解跳出局部最优，提升个体质量，但全局搜索能力有限。
- **融合两者**，可以兼顾全局探索与局部精细优化，提高收敛速度和最终解的质量。

#### 2. 融合的基本方法

- 以遗传算法为主导，负责种群的全局进化（选择、交叉、变异）。
- 在每一代GA进化后，对种群中的每个个体，使用模拟退火进行局部搜索，进一步提升个体质量。
- 这样既能保持多样性和全局搜索能力，又能让每个个体在其邻域内充分优化。

#### 3. 具体实现流程

1. **初始化种群**：用多组初始配置和随机个体生成初始种群。
2. **主循环（多代进化）**：
   - 评估每个个体的适应度（如HPL-AI性能分数）。
   - 记录和更新全局最优解。
   - 统计并输出当前代的最优、平均、最差适应度。
   - **对每个个体，执行模拟退火局部搜索**：
     - 以当前个体为起点，做若干步小扰动（变异），每步以一定概率接受较差解。
     - 搜索过程中记录遇到的最优解，若优于原个体则替换。
     - 若新个体优于全局最优，则更新全局最优。
   - 除最后一代外，执行GA的选择、交叉、变异生成新一代。
3. **输出最优配置和优化历史**。

#### 具体代码实现

首先，定义模拟退火相关函数simulated_annealing_local_search，符合模块化开发规范。

```python
def simulated_annealing_local_search(
        self, individual, steps=5, T0=1.0, cooling=0.9, individual_id_prefix="sa"
    ):
        """
        对单个个体进行模拟退火局部搜索
        """
        current = copy.deepcopy(individual)
        current_fitness = self.evaluate_fitness(current, f"{individual_id_prefix}_0")
        best = copy.deepcopy(current)
        best_fitness = current_fitness
        T = T0
        for step in range(1, steps + 1):
            neighbor = self.mutate(current)
            neighbor_fitness = self.evaluate_fitness(
                neighbor, f"{individual_id_prefix}_{step}"
            )
            delta = neighbor_fitness - current_fitness
            if delta < 0 or random.random() < math.exp(-delta / T):
                current = neighbor
                current_fitness = neighbor_fitness
                if current_fitness < best_fitness:
                    best = copy.deepcopy(current)
                    best_fitness = current_fitness
            T *= cooling
        return best, best_fitness
```

接着，在算法主流程中加入模拟退火的模块，对每个种群的个体进行模拟退火操作：

```python
# === GA+SA混合：对每个个体做模拟退火局部搜索 ===
for i, individual in enumerate(population):
    improved, improved_fitness = self.simulated_annealing_local_search(
        individual,
        steps=5,        # 模拟退火步数，控制局部搜索的深度
        T0=1.0,         # 初始温度，影响接受较差解的概率
        cooling=0.9,    # 降温系数，每步温度乘以该值，控制温度下降速度
        individual_id_prefix=f"gen{generation}_ind{i}_sa",  # 评估ID前缀，用于区分不同个体的SA搜索
    )
    if improved_fitness < fitness_values[i]:
        population[i] = improved
        fitness_values[i] = improved_fitness
        # 更新最佳配置
        if improved_fitness < self.best_fitness:
            self.best_fitness = improved_fitness
            self.best_config = copy.deepcopy(improved)
            self.best_generation = generation
            self.best_index = i
            print(f"SA提升了个体 {i}，新适应度: {-improved_fitness:.4f}")
# === End GA+SA混合 ===
```

#### 预期效果与实际表现

- **理论预期**：融合后，优化过程既能全局探索，又能局部精细提升，能更快找到高质量解，避免陷入局部最优。

- **实际表现**（以HPL-AI性能为例）：

##### 性能对比表

| 配置类型 | Passing Rate | 最小性能 (Gflops) | 平均性能 (Gflops) | 最大性能 (Gflops) | 性能提升倍数 |
|---------|-------------|------------------|------------------|------------------|-------------|
| 基准配置 | 100.00% | 0.0490 | 0.0631 | 0.0698 | 1.0x |
| 优化后配置 | 100.00% | 0.2557 | 0.2954 | 0.3187 | 4.7x |

##### 性能提升分析

**基准配置**（pre.log）：
```
Passing Rate: 100.00%
n = 100, Performance =   0.0490 Gflops, MemUsage =   9.5984 MB
n = 125, Performance =   0.0623 Gflops, MemUsage =   9.7520 MB
n = 150, Performance =   0.0659 Gflops, MemUsage =   9.9472 MB
n = 175, Performance =   0.0685 Gflops, MemUsage =  10.0568 MB
n = 200, Performance =   0.0698 Gflops, MemUsage =  10.3136 MB
Smallest/Average/Largest Performance =   0.0490 Gflops,   0.0631 Gflops,   0.0698 Gflops
```

**优化后配置**（179-variables.txt）：
```
Passing Rate: 100.00%
n = 100, Performance =   0.2557 Gflops, MemUsage =   9.4744 MB
n = 125, Performance =   0.2915 Gflops, MemUsage =   9.9096 MB
n = 150, Performance =   0.3011 Gflops, MemUsage =  10.4984 MB
n = 175, Performance =   0.3102 Gflops, MemUsage =  10.9144 MB
n = 200, Performance =   0.3187 Gflops, MemUsage =  11.8360 MB
Smallest/Average/Largest Performance =   0.2557 Gflops,   0.2954 Gflops,   0.3187 Gflops
```

##### 关键指标对比

```
性能提升效果：
├── 平均性能：0.0631 → 0.2954 Gflops (提升 4.7倍)
├── 最大性能：0.0698 → 0.3187 Gflops (提升 4.6倍)
├── 最小性能：0.0490 → 0.2557 Gflops (提升 5.2倍)
└── 通过率：100.00% (保持不变)
```

- **提升说明**：融合优化后，平均性能提升了约4.7倍，且通过率保持100%。这体现了GA+SA混合优化的有效性。

#### 对比图

##### 性能对比图 (performance_comparison.png)

![性能对比图](https://i-blog.csdnimg.cn/direct/210ab905142142c1a61d7f0f14c59bb4.png#pic_center)


- 展示基准配置与优化后配置的性能对比
- 包含最小、平均、最大性能指标
- 显示性能提升倍数
- **详细解读**：
  - 该图直观对比了优化前后在HPL-AI基准下的性能表现。可以看到，GA+SA混合优化后，无论是最小、平均还是最大性能，均有显著提升。
  - 右侧的提升倍数图进一步量化了优化效果，便于一眼看出各项指标的提升幅度。
  - 通过该图，可以快速评估混合优化算法在实际应用中的性能收益，为后续算法选择和工程落地提供数据支撑。

##### 优化流程图 (optimization_flow.png)

![优化流程图](https://i-blog.csdnimg.cn/direct/9d9a770f6fa24ed2bfa385dcfb8dd1d2.png#pic_center)


- 详细展示GA+SA混合优化的完整流程
- 包含初始化、GA进化、SA局部搜索、更新输出等阶段
- 用不同颜色区分各个阶段（浅蓝色-初始化，浅红色-GA进化，浅绿色-SA局部搜索，浅橙色-更新输出）
- **详细解读**：
  - 流程图清晰地展示了混合优化的各个步骤及其相互关系，便于理解算法的整体架构。
  - 主流程包括：初始化种群（多配置+随机）→ 评估适应度（HPL-AI性能）→ 选择（锦标赛）→ 交叉（精度交换）→ 变异（随机改变）→ SA局部搜索（对每个个体）→ 生成邻居（小扰动）→ 概率接受 → 更新个体（替换更好）→ 更新全局最优 → 检查终止条件 → 输出最佳配置和优化历史
  - 箭头的设计避免了穿插，主流程自上而下，回环贴边，突出主流程的连续性和循环结构。
  - 该图适合用于算法讲解、团队交流、论文/答辩展示，有助于快速把握GA+SA混合优化的核心思想和实现逻辑。

##### 收敛曲线对比图 (convergence_comparison.png)

![收敛曲线对比啊](https://i-blog.csdnimg.cn/direct/68825aa4c19b46888f83fd089080f771.png#pic_center)


- 折线图对比了GA和GA+SA两种算法在优化过程中的最优适应度（Best Fitness）和平均适应度（Avg Fitness）随迭代代数的变化趋势。
- 图中用不同颜色和标记区分不同算法和曲线，并加有注释说明。
- **详细解读**：
  - 该图反映了两种算法的收敛速度和最终性能。可以看到，GA+SA混合优化不仅收敛更快，而且最终能达到更高的性能水平。
  - 平均适应度的提升说明整体种群质量也得到了改善，而不仅仅是个别最优解。
  - 通过该图，可以直观比较不同优化策略的效果，为算法改进和参数调优提供依据。

##### 参数分析图 (parameter_analysis.png)

![参数分析图](https://i-blog.csdnimg.cn/direct/8919eba995144d2f87d5fce6853856fe.png#pic_center)


- 四个子图分别分析了SA步数、初始温度T0、降温系数、种群大小等关键参数对最终性能和计算开销的影响。
- 每个子图都用不同颜色和标记区分性能与时间等指标。
- **详细解读**：
  - 该图揭示了各关键参数对优化效果和资源消耗的影响规律。例如，SA步数增加会提升性能但也增加计算时间，初始温度和降温系数的选择会影响跳出局部最优的能力，种群大小影响全局搜索能力和收敛速度。
  - 通过这些分析，可以为实际应用中参数的合理选择和算法调优提供科学依据，避免盲目试错。
  - 该图适合用于技术报告、参数敏感性分析和工程实践指导。


## 配置去重

### 什么是哈希值

#### 定义

通过**哈希函数（Hash Function）** 对任意长度的数据（如字符串、文件、配置等）进行计算后，得到的一个**定长的唯一标识符**（通常是一个十六进制字符串）。

***就是用来唯一标识一段数据的“指纹”***

#### 特点

- 唯一
- **定长：** 无论输入，输出长度一定
- 不可被反解码，即不可逆
- 计算速度快

#### 常见哈希算法

- **MD5：** 常用，输出32位十六进制字符串
- **SHA-1、SHA-256：** 更安全，输出更长
- **CRC32：** 常用于校验

### 什么是配置去重

#### 核心思想

- 为每个配置生成哈希值并存储
- 每次生成新配置的时候就先察哈希表，存在则跳过/丢弃或重新生成

#### 优势

如果不去重，同一个配置可能被多次测试，浪费大量计算资源，降低搜索效率。

---

#### 实现细节

##### 1. 配置哈希生成

- 对每个变量，拼接 function.name:type 字符串
- 全部变量按字典序排序，拼成一个长字符串
- 用 **md5** 算法生成哈希值（32位十六进制字符串）
- 这样只要变量分配完全一样，哈希值就一样

```python
def get_config_hash(self, config: Dict[str, Any]) -> str:
    var_types = []
    for var in config.get("localVar", []):
        var_types.append(f"{var['function']}.{var['name']}:{var['type']}")
    var_types.sort()
    config_str = "|".join(var_types)
    import hashlib
    return hashlib.md5(config_str.encode()).hexdigest()
```

##### 2. 去重存储与检查

1. 用 self.tested_configs（一个 set）存所有已测试配置的哈希值
2. 每次生成新配置、变异、交叉、初始化时，都用 is_config_tested 检查哈希是否已存在
3. 新配置测试前用 mark_config_as_tested 标记

##### 3. 缓存持久化

- 所有已测试配置的哈希和详情会定期保存到 tested_configs_cache.json
- 支持断点续测，重启后不会重复测试历史配置
- 支持清理、导出、查看缓存

##### 4. 种群生成与进化中的去重

- **create_initial_population：** 初始种群只添加未测试过的配置
- **create_random_individual：** 生成新个体时最多尝试1000次，确保唯一
- **evolve_population：** 交叉、变异后也检查去重，不够就用新随机个体补齐

##### 5. 适应度评估中的去重

- **evaluate_fitness：** 评估前先标记配置为已测试
- 如果发现配置已测试过，可以直接返回无穷大适应度，跳过实际测试

#### 流程图

```mermaid

flowchart TD
    A[生成新配置] --> B{计算哈希}
    B --> C{哈希是否已存在?}
    C -- 是 --> D[丢弃/重新生成]
    C -- 否 --> E[加入种群/测试]
    E --> F[测试后标记为已测试]





