***AI-深度学习Transformer***

# 什么是注意力？

想象你在看一句话：“我爱吃苹果”。

1. 给每个字发三张“小纸条”：
    - 一张写“我要找谁”——叫 Q
    - 一张写“我是谁”——叫 K
    - 一张写“我有啥信息”——叫 V

2. 现在让“果”字做例子：

它拿自己的 Q 去跟所有字的 K 对眼神，发现“苹”的 K 最对得上，得分高；其它字得分低。

把得分变成百分比（softmax），相当于“果”把注意力 90% 放在“苹”上，其余字忽略不计。

3. 最后“果”把各字的 V 按注意力百分比加权混合：

90% 拿“苹”的 V，10% 拿别的，混成一个新的“果”向量。

这个新向量既保留了“果”自己的信息，又重点融合了“苹”的信息，于是模型知道“苹果”是一个整体。

4. 所有字同时做这套“对眼神→加权混合”，一句话就一次性更新完毕，不用像 RNN 那样挨个读。

就这么简单：

“谁跟我相关，我就多看看谁，把它的信息搬到我这里。”

整句话一起算，又快又能抓远距离关系，这就是“Attention is all you need”的通俗版。

***这就是“注意力机制”最核心的思想：在处理当前信息时，根据重要性，有选择地从所有相关信息中提取关键部分。***

# Transformer组件

## 自注意力机制 (Self-Attention) - “全局关联器”

让句子（或序列）中的每个词都能直接和所有其他词进行“交流”，从而找到它们之间的关系。

技术简化的三步流程：

- 提问与回答：每个词生成三个东西：一个问题（Query）、一个答案摘要（Key）和一份完整答案（Value）。
- 匹配打分：对于当前词（比如“它”），拿它的“问题”去和所有词的“答案摘要”比较，得出匹配分数（注意力权重）。
- 汇总信息：用这些分数作为比例，对所有词的“完整答案”进行加权混合，得到一个新的、融合了全局信息的“它”。
