***AI-深度学习Transformer***

# 什么是注意力？

想象你在看一句话：“我爱吃苹果”。

1. 给每个字发三张“小纸条”：
    - 一张写“我要找谁”——叫 Q
    - 一张写“我是谁”——叫 K
    - 一张写“我有啥信息”——叫 V

2. 现在让“果”字做例子：

它拿自己的 Q 去跟所有字的 K 对眼神，发现“苹”的 K 最对得上，得分高；其它字得分低。

把得分变成百分比（softmax），相当于“果”把注意力 90% 放在“苹”上，其余字忽略不计。

3. 最后“果”把各字的 V 按注意力百分比加权混合：

90% 拿“苹”的 V，10% 拿别的，混成一个新的“果”向量。

这个新向量既保留了“果”自己的信息，又重点融合了“苹”的信息，于是模型知道“苹果”是一个整体。

4. 所有字同时做这套“对眼神→加权混合”，一句话就一次性更新完毕，不用像 RNN 那样挨个读。

就这么简单：

“谁跟我相关，我就多看看谁，把它的信息搬到我这里。”

整句话一起算，又快又能抓远距离关系，这就是“Attention is all you need”的通俗版。

***这就是“注意力机制”最核心的思想：在处理当前信息时，根据重要性，有选择地从所有相关信息中提取关键部分。***

# Transformer流程

```
输入：[batch_size, sequence_length]
    │
    ▼
词嵌入层 (Embedding) → [batch_size, seq_len, hidden_dim]
    │
    + 位置编码 (Positional Encoding)
    │
    ▼
输入张量：[batch_size, seq_len, hidden_dim]
    │
┌─────────────────────────────────────────┐
│        N个Transformer Encoder Layer      │
│  (每个layer结构完全相同，但参数不共享)     │
└─────────────────────────────────────────┘
    │
    ▼
编码器输出：[batch_size, seq_len, hidden_dim]
    │
    ▼
可选：池化、分类头等下游任务层
```

## 词嵌入层

### 作用

将离散的词索引（整数）转换为连续的稠密向量。

### 实现方式

创建一个可学习的矩阵：[词表大小, 隐藏维度]

```
词表：{"猫":0, "狗":1, "鱼":2, "吃":3}
隐藏维度：4

嵌入矩阵（随机初始化）：
[[0.1, 0.2, 0.3, 0.4],  # 索引0:"猫"
 [0.5, 0.6, 0.7, 0.8],  # 索引1:"狗" 
 [0.9, 1.0, 1.1, 1.2],  # 索引2:"鱼"
 [1.3, 1.4, 1.5, 1.6]]  # 索引3:"吃"

输入句子："猫 吃 鱼" → 索引：[0, 3, 2]
输出：
[[0.1, 0.2, 0.3, 0.4],  # "猫"
 [1.3, 1.4, 1.5, 1.6],  # "吃"
 [0.9, 1.0, 1.1, 1.2]]  # "鱼"
形状：[3, 4]
```

## 位置编码

### 作用

为序列中的每个位置添加位置信息，因为自注意力本身没有位置概念。

### 实现方式

实际上就是加一个形状相同的矩阵(可学习)：

```
位置0的编码：[sin(0), cos(0), sin(0/100), cos(0/100)] = [0.0, 1.0, 0.0, 1.0]
位置1的编码：[sin(1), cos(1), sin(1/100), cos(1/100)] = [0.84, 0.54, 0.01, 1.0]
位置2的编码：[sin(2), cos(2), sin(2/100), cos(2/100)] = [0.91, -0.42, 0.02, 1.0]

与词嵌入相加：
"猫"(位置0)的词向量：[0.1, 0.2, 0.3, 0.4]
+ 位置0编码：[0.0, 1.0, 0.0, 1.0]
= [0.1, 1.2, 0.3, 1.4]
```

## Transformer Encoder Layer

每个Layer包含：

### 子层1：多头自注意力 (Multi-Head Self-Attention)

```
输入: X [batch_size, seq_len, hidden_dim]
    |
    ▼
层归一化 (可选，取决于Pre-LN还是Post-LN)
    |
    ▼
线性投影 → Q, K, V [batch_size, seq_len, hidden_dim]
    |
    ▼
重排/分割 → [batch_size, num_heads, seq_len, head_dim]
    |          (head_dim = hidden_dim / num_heads)
    ▼
缩放点积注意力 (Scaled Dot-Product Attention):
    Q × K^T → [batch_size, num_heads, seq_len, seq_len]
    |
    ▼
缩放: ÷ sqrt(head_dim)
    |
    ▼
Softmax (可选+掩码)
    |
    ▼
× V → [batch_size, num_heads, seq_len, head_dim]
    |
    ▼
重排/合并 → [batch_size, seq_len, hidden_dim]
    |
    ▼
输出投影 (线性层)
    |
    ▼
Dropout + 残差连接 (残差: + 子层输入)
```

#### 第一步：层归一化 (Layer Normalization)

对每个单词向量单独进行标准化，使其均值为0，方差为1，然后进行缩放和偏移。

**以"猫"向量 [0.5, 1.2, -0.3, 0.8] 为例：**

***计算均值和标准差：***

```
均值 μ = (0.5 + 1.2 + (-0.3) + 0.8) / 4 = 2.2 / 4 = 0.55

方差 = [(0.5-0.55)² + (1.2-0.55)² + (-0.3-0.55)² + (0.8-0.55)²] / 4
     = [0.0025 + 0.4225 + 0.7225 + 0.0625] / 4
     = 1.21 / 4 = 0.3025

标准差 σ = √0.3025 = 0.55
```

***归一化：***

```
归一化后 = (原始值 - 均值) / (标准差 + ε)  # ε=1e-5防止除零
        = [(0.5-0.55)/0.55, (1.2-0.55)/0.55, (-0.3-0.55)/0.55, (0.8-0.55)/0.55]
        = [-0.0909, 1.1818, -1.5455, 0.4545]
```

- **稳定训练**：深层网络中，每层的输入分布会变化（称为"内部协变量偏移"），导致训练不稳定
- **加速收敛**：标准化后的数据在激活函数的敏感区域（如tanh的线性区），梯度更大
- **减少对初始化的依赖**：不管输入多大，都归一化到相似范围
- **训练/推理一致**：不像BatchNorm需要running mean，LayerNorm在训练和推理时行为一致

***缩放和偏移：***

```
假设可学习参数：
γ = [1.0, 1.0, 1.0, 1.0]  # 缩放参数
β = [0.0, 0.0, 0.0, 0.0]  # 偏移参数

最终输出 = γ × 归一化后 + β
        = [1.0×(-0.0909), 1.0×1.1818, 1.0×(-1.5455), 1.0×0.4545]
        = [-0.0909, 1.1818, -1.5455, 0.4545]
```

#### 第二步：多头自注意力 (Multi-Head Self-Attention)

***分解***

对于归一化之后的“猫”向量进行维度上面的切分，分为多个头，每个头并行进行注意力，且各自有自己的W，分别关注不同方面的信息

***线性投影得到Q, K, V***

```cpp
W_q = [[0.1, 0.2],
       [0.3, 0.4],
       [0.5, 0.6],
       [0.7, 0.8]]
形状：[4, 2]  # 4维输入 → 2维输出

W_k = [[0.2, 0.1],
       [0.4, 0.3],
       [0.6, 0.5],
       [0.8, 0.7]]

W_v = [[0.3, 0.4],
       [0.5, 0.6],
       [0.7, 0.8],
       [0.9, 1.0]]

// 计算"猫"的Q向量：

"猫"归一化后：[-0.0909, 1.1818, -1.5455, 0.4545]

Q_猫 = X_猫 × W_q
     = -0.0909×0.1 + 1.1818×0.3 + (-1.5455)×0.5 + 0.4545×0.7   # 第一维
     = -0.00909 + 0.35454 + (-0.77275) + 0.31815 = -0.10915
     
     第二维：-0.0909×0.2 + 1.1818×0.4 + (-1.5455)×0.6 + 0.4545×0.8
     = -0.01818 + 0.47272 + (-0.9273) + 0.3636 = -0.10916

所以 Q_猫 = [-0.10915, -0.10916]
```

- **不同角色**：Query负责"提问"，Key负责"回答问题"，Value负责"提供信息"
- **灵活性**：让模型可以学习不同的表示方式
- **表达能力**：三个空间独立，可以关注不同的特征

***计算注意力分数***

```cpp
1. 猫的Query Q_猫 = [-0.10915, -0.10916]
2. 所有词的Key：
   K_猫 = [-0.10915, -0.10916]  (计算方式同Q)
   K_吃 = [0.6273, 0.4818]
   K_鱼 = [-0.8182, -0.6545]
   
3. 计算点积（相关性）：
   猫-猫：[-0.10915, -0.10916]·[-0.10915, -0.10916] = 0.0119 + 0.0119 = 0.0238
   猫-吃：[-0.10915, -0.10916]·[0.6273, 0.4818] = -0.0685 + (-0.0526) = -0.1211
   猫-鱼：[-0.10915, -0.10916]·[-0.8182, -0.6545] = 0.0893 + 0.0715 = 0.1608
```
- **动态权重**：不是固定权重，而是根据内容动态决定关注谁
- **上下文感知**：同一个词在不同上下文中会关注不同的词
- **长距离依赖**：可以直接关注到很远的词，不像RNN需要一步步传递

***缩放和Softmax***

```
// 1. 缩放：除以√d_k（这里d_k=2，√2≈1.414）

猫的分数：0.0238/1.414 = 0.0168
         -0.1211/1.414 = -0.0856
          0.1608/1.414 = 0.1137

// 2. Softmax：转换为概率分布（和为1）

exp(0.0168) = 1.0169
exp(-0.0856) = 0.9180
exp(0.1137) = 1.1204
总和 = 1.0169 + 0.9180 + 1.1204 = 3.0553

注意力权重：
猫关注猫：1.0169/3.0553 = 0.333
猫关注吃：0.9180/3.0553 = 0.300
猫关注鱼：1.1204/3.0553 = 0.367
```

1. **为什么缩放？**
    - 当维度`d_k`很大时，点积结果可能很大
    - 这会使softmax梯度很小（梯度消失）
    - 除以√d_k让方差保持在1左右
2. **为什么需要Softmax？**
    - 将分数转换为概率分布
    - 确保所有权重和为1，可以理解为"注意力分配比例"

***加权求和（乘V）***

用注意力权重对Value向量进行加权求和。

```
1. 所有词的Value向量：
   V_猫 = [0.0546, 0.0728]
   V_吃 = [1.0455, 1.2727]
   V_鱼 = [-0.8182, -0.9091]
   
2. 加权求和：
   新猫向量 = 0.333×V_猫 + 0.300×V_吃 + 0.367×V_鱼
           = 0.333×[0.0546,0.0728] + 0.300×[1.0455,1.2727] + 0.367×[-0.8182,-0.9091]
           = [0.0182,0.0242] + [0.3137,0.3818] + [-0.3003,-0.3337]
           = [0.0316, 0.0723]
```
- **信息融合**：将其他词的信息融合到当前词
- **软注意力**：不是硬性选择，而是软性混合
- **可微分**：整个过程可微分，可以端到端训练

***重排/合并+输出投影 (线性层)***

先进行简单的concat，然后乘w加b

#### 第三步：残差连接

将自注意力层的输出加上原始的输入（归一化前的输入）。

```
残差连接 = 自注意力输出 + 原始输入
         = [新猫向量] + [0.5, 1.2, -0.3, 0.8]
```
- **梯度高速公路**：梯度可以直接通过加法传回去，缓解梯度消失
- **信息保护**：即使本层没学到有用信息，至少还能保留输入
- **恒等映射**：网络可以轻松学习"什么也不做"（输出=输入）

### 子层2：前馈神经网络 (Feed-Forward Network)

```
class FeedForward(nn.Module):
    def __init__(self, hidden_dim=512, ff_dim=2048):
        super().__init__()
        # 标准配置：ff_dim = 4 * hidden_dim
        self.linear1 = nn.Linear(hidden_dim, ff_dim)
        self.linear2 = nn.Linear(ff_dim, hidden_dim)
        self.activation = nn.GELU()  # 或ReLU
        
    def forward(self, x):
        # x: [batch_size, seq_len, hidden_dim]
        # 对序列中每个位置独立应用相同的前馈网络
        return self.linear2(self.activation(self.linear1(x)))
```

对每个位置的向量独立进行非线性变换。

```
第一层：4维 → 8维（升维）
第二层：8维 → 4维（降维）
激活函数：ReLU（负值变0）

以"猫"的残差输出 [0.5316, 1.2723, -0.2684, 0.8723] 为例：
```

#### 第一步：层归一化

再次归一化，步骤同第一步

#### 第二步：第一层线性变换

```
假设权重W1: [4×8]，偏置b1: [8]
计算：x × W1 + b1 → 8维向量
假设得到：[0.2, -0.5, 1.3, 0.8, -0.1, 0.4, 0.9, 0.2]
```

#### 第三步：ReLU激活

```
ReLU([0.2, -0.5, 1.3, 0.8, -0.1, 0.4, 0.9, 0.2])
= [0.2, 0, 1.3, 0.8, 0, 0.4, 0.9, 0.2]  # 负值变0
```

#### 第四步：第二层线性变换

```
假设权重W2: [8×4]，偏置b2: [4]
计算 → 4维向量，假设得到：[0.4, 0.9, -0.2, 0.7]
```

- **增加非线性**：自注意力主要是线性操作+softmax，FFN提供更强的非线性能力
- **位置独立处理**：每个位置独立处理，不会混合位置信息
- **维度变换**：先升维再降维，形成"瓶颈"，增加模型容量

#### 第五步：最终残差连接

将FFN的输出加上FFN的输入。

```
FFN输入（层归一化后的残差输出）：[x1, x2, x3, x4]
FFN输出：[0.4, 0.9, -0.2, 0.7]
最终输出 = [0.4+x1, 0.9+x2, -0.2+x3, 0.7+x4]
```

# AI编译器的视角

```
输入张量 X: [B, S, D] (Batch, Sequence, Hidden)
│
├─── 子图A：多头自注意力 (MHSA) ────────────── 核心计算密集型区域
│   │
│   ├─ 1. 层归一化 (Pre-Norm)
│   │   └→ X_norm: [B, S, D]                      // 逐点操作，访存密集
│   │
│   ├─ 2. 投影与重排 (Q/K/V Projection & Split)
│   │   ├→ Q = Linear(X_norm, W_q): [B, S, D]
│   │   ├→ K = Linear(X_norm, W_k): [B, S, D]
│   │   ├→ V = Linear(X_norm, W_v): [B, S, D]    // 三个独立GEMM，计算密集
│   │   │
│   │   ├→ Q_reshaped: [B, S, H, Dh]→[B, H, S, Dh] // H:头数，Dh=D/H
│   │   ├→ K_reshaped: [B, S, H, Dh]→[B, H, S, Dh] // 数据重排，引入转置
│   │   └→ V_reshaped: [B, S, H, Dh]→[B, H, S, Dh] // 内存不连续访问
│   │
│   ├─ 3. 注意力核心计算
│   │   ├→ S = Q @ K^T / sqrt(Dh): [B, H, S, S]   // 大GEMM，O(S²)复杂度
│   │   ├→ P = Softmax(S, dim=-1): [B, H, S, S]   // 逐点规约，访存密集
│   │   └→ O_heads = P @ V: [B, H, S, Dh]         // 另一个大GEMM
│   │
│   ├─ 4. 合并与输出投影
│   │   ├→ O_concat: [B, H, S, Dh]→[B, S, H, Dh]→[B, S, D] // 反向重排+拼接
│   │   └→ Attn_Out = Linear(O_concat, W_o): [B, S, D] // 最终GEMM
│   │
│   └─ 5. 残差连接
│       └→ Z = X + Attn_Out: [B, S, D]            // 逐点加法，访存密集
│
├─── 子图B：前馈网络 (FFN) ────────────────────── 另一核心计算区
│   │
│   ├─ 1. 层归一化 (Pre-Norm)
│   │   └→ Z_norm: [B, S, D]                      // 逐点操作
│   │
│   ├─ 2. 两层全连接层
│   │   ├→ FF1 = GELU(Linear(Z_norm, W1)): [B, S, 4D] // GEMM + 逐点激活
│   │   └→ FF2 = Linear(FF1, W2): [B, S, D]       // GEMM
│   │
│   └─ 3. 残差连接
│       └→ Layer_Out = Z + FF2: [B, S, D]         // 逐点加法
│
└─── 输出: [B, S, D] (作为下一层输入)
```