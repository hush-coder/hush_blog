# DD的整体流程

```
输入图片 [3, H, W]
    ↓
CNN骨干网络（如ResNet-50）
    ↓
多尺度特征金字塔（4个尺度）
    ├── 下采样4倍：[C, H/4, W/4]  ← 细节丰富，看小物体
    ├── 下采样8倍：[C, H/8, W/8]
    ├── 下采样16倍：[C, H/16, W/16]
    └── 下采样32倍：[C, H/32, W/32] ← 全局信息，看大物体
    ↓
Transformer编码器（6层）
    ├── 每层：多尺度可变形注意力(MSDA)
    └── 输出：增强的多尺度特征
    ↓
Transformer解码器（6层）
    ├── 每层：
    │   ├── 自注意力（Query之间交流）
    │   └── 交叉注意力（MSDA：Query看特征图）
    └── 输出：100个Query的最终特征
    ↓
预测头（FFN）
    ├── 分类分支 → 100个预测类别（91类 + 空类）
    └── 回归分支 → 100个边界框坐标
    ↓
匈牙利匹配（训练时）→ 计算损失
最终输出（推理时）：过滤空类 → 得到检测结果
```

## 第1步：特征提取（CNN骨干网络）

```python
输入：图像 [batch_size, 3, 800, 600]  # 假设800×600
输出：4个多尺度特征图

# CNN通常取4个阶段的输出：
stage1_out = [batch_size, 256, 200, 150]   # 下采样4倍
stage2_out = [batch_size, 512, 100, 75]    # 下采样8倍
stage3_out = [batch_size, 1024, 50, 37]    # 下采样16倍
stage4_out = [batch_size, 2048, 25, 18]    # 下采样32倍

# 用1×1卷积统一通道数到256
for each stage:
    feature = 1x1_conv(feature)  # 都变成 [batch_size, 256, H, W]
```
***为什么需要多尺度？***
- **小物体（远处的人）**：在下采样4倍的特征图上清晰
- **大物体（近处的车）**：在下采样32倍的特征图上能看到整体

## 第2步：Transformer编码器（6层）

**编码器的任务**：让特征图自己"思考"，增强特征表示

```
输入特征图 → 层归一化 → MSDA（多尺度可变形注意力） → Add → 
层归一化 → FFN（前馈网络） → Add → 输出特征图
```
**关键**：编码器中的MSDA是特征图位置之间的注意力！
- 每个特征图位置是一个"查询"(query)
- 每个位置预测要关注其他哪些位置的特征
- 让特征图中的信息充分交互

## 第3步：Transformer解码器（6层）

**解码器的任务**：让100个Query逐步学习"看图找物"

```
输入Query → 层归一化 → 自注意力 → Add → 
层归一化 → 交叉注意力(MSDA) → Add → 
层归一化 → FFN → Add → 输出Query
```
***两个关键注意力：***
- **自注意力**：Query之间交流信息
- **交叉注意力(MSDA)**：Query看特征图（这是核心！）


## 第4步：预测头（FFN）

```
# 每个Query最终输出一个预测
for each query in 100 queries:
    # 分类分支：这是什么？
    class_logits = linear_class(query)  # [91+1]维，+1是空类
    
    # 回归分支：框在哪里？
    bbox_coords = linear_bbox(query)  # [4]维，(cx, cy, w, h)
    
    # 注意：预测的是相对参考点的偏移！
    final_bbox = reference_point + bbox_coords
```

# 跟踪实例

假设我们有一张3×3的简单猫图片（为了演示简化）

```
原图（3×3，RGB）：
像素坐标     R    G    B
(0,0)     100  50   20   ← 猫耳朵（橙色）
(0,1)     120  70   30   ← 猫耳朵
(0,2)     80   40   10   ← 背景
(1,0)     90   60   25   ← 猫脸
(1,1)     110  80   40   ← 猫眼睛（中心）
(1,2)     70   30   5    ← 背景
(2,0)     85   55   22   ← 猫下巴
(2,1)     95   65   28   ← 猫脖子
(2,2)     60   20   0    ← 背景
```

## 第1步：图片输入

***注意：此时输入张量为`[批次, 通道, 高, 宽] = [1, 3, 3, 3]`***

```python
# 输入张量形状：[批次, 通道, 高, 宽] = [1, 3, 3, 3]
input_image = torch.tensor([[
    [[[100, 120, 80],    # 第0行，RGB通道
       [90, 110, 70],    
       [85, 95, 60]]],   # R通道（红色）
     
    [[[50, 70, 40],      # G通道（绿色）
       [60, 80, 30],
       [55, 65, 20]]],
     
    [[[20, 30, 10],      # B通道（蓝色）
       [25, 40, 5],
       [22, 28, 0]]]
]])
```
> **此时**：计算机看到的是原始像素值（0-255），没有"猫"的概念。

## 第2步：第一层卷积（提取边缘/纹理）

假设第一层有4个卷积核（4种"检测器"）：

```cpp
// 卷积核1：检测"左上到右下的边缘"
kernel1 = [[ 1,  0, -1],
           [ 0,  0,  0],
           [-1,  0,  1]]

// 卷积核2：检测"垂直边缘"
kernel2 = [[-1,  0,  1],
           [-1,  0,  1],
           [-1,  0,  1]]

// 卷积核3：检测"水平边缘"
kernel3 = [[-1, -1, -1],
           [ 0,  0,  0],
           [ 1,  1,  1]]

// 卷积核4：检测"角点"
kernel4 = [[ 1, -1, -1],
           [-1,  1, -1],
           [-1, -1,  1]]
```

**计算过程**（以R通道 + kernel1为例）：

```
R通道的3×3区域（以(1,1)为中心）：
[100, 120, 80]
[90, 110, 70]
[85, 95, 60]

卷积计算：
100×1 + 120×0 + 80×(-1) +
90×0 + 110×0 + 70×0 +
85×(-1) + 95×0 + 60×1
= 100 - 80 - 85 + 60 = -5
```
> 由此可得一个[3, 3]的矩阵

对每个位置、每个通道、每个卷积核都计算：

***注意：此时第一个特征图为`[批次, 通道（对应4个卷积核）, 高, 宽] = [1, 4, 3, 3]`***

```python
# 第一个特征图（简化值）：
feature_map1_channel1 = [[-5, 10, 15],   # kernel1的输出
                         [8, -5, 12],
                         [7, 9, -3]]
                         
feature_map1_channel2 = [[-8, 5, 12],    # kernel2的输出
                         [6, -8, 10],
                         [5, 7, -5]]
                         
feature_map1_channel3 = [[-3, 8, 10],    # kernel3的输出
                         [7, -3, 9],
                         [6, 8, -2]]

feature_map1_channel4 = [[-2, 6, 8],     # kernel4的输出
                         [5, -2, 7],
                         [4, 6, -1]]
```
> **此时**：我们得到了4个特征图，每个检测不同的边缘/纹理模式。

## 第3步：激活函数(ReLU)

```python
# ReLU：把所有负数变成0，正数保持不变

feature_map1_channel1_after_relu = [[0, 10, 15],   # -5变成0
                                    [8, 0, 12],
                                    [7, 9, 0]]
```
> *为什么？*因为负值通常表示"**没有这个特征**"
> **直觉**：只保留"有这种特征"的位置，去掉"没有这种特征"的位置。

## 第4步：池化（下采样）

假设用2×2最大池化，步长2：

```python
# 对feature_map1_channel1_after_relu：
[[0, 10, 15],
 [8, 0, 12],
 [7, 9, 0]]

# 分成2×2的区域，取最大值：
# 区域1: [[0,10], [8,0]] → 最大值=10
# 区域2: [[15], [12]] → 最大值=15
# 区域3: [[7,9]] → 最大值=9
# 区域4: [[0]] → 最大值=0

# 输出：[1, 4, 2, 2]  ← 尺寸从3×3变成2×2
pooled_feature_map1_channel1 = [[10, 15],
                                [9, 0]]
```
> 减少计算量（特征图变小）
> 增加感受野（每个像素看到更大区域）
> 增加平移不变性（物体移动一点，特征还在）

## 第5步：多层卷积堆叠

现在我们有4个2×2的特征图。再通过第二层卷积：

```python
# 每个卷积核要看第一层的所有4个通道！
# 这样能组合第一层的简单特征，形成复杂特征

kernel5 = [[[0.1, 0.2, 0.3],   # 看channel1的权重
            [0.4, 0.5, 0.6],
            [0.7, 0.8, 0.9]],
           
           [[-0.1,-0.2,-0.3],  # 看channel2的权重
            [-0.4,-0.5,-0.6],
            [-0.7,-0.8,-0.9]],
           
           [[0.2, 0.1, 0.3],   # 看channel3的权重
            [0.5, 0.4, 0.6],
            [0.8, 0.7, 0.9]],
           
           [[-0.2,-0.1,-0.3],  # 看channel4的权重
            [-0.5,-0.4,-0.6],
            [-0.8,-0.7,-0.9]]]
```

**计算**：在位置(0,0)，卷积核5要看：
- 第一层4个特征图的(0,0)为中心的3×3区域
- 由于padding，实际可能看补0后的区域
**直觉**：第二层卷积核能检测"边缘的组合"，比如：
- 卷积核5：检测"两个垂直边缘中间有个角点"（可能是猫眼睛）
- 卷积核6：检测"水平边缘上有纹理"（可能是猫毛皮）

## 第6步：残差连接（ResNet的关键）

```python
# 假设输入是X，卷积输出是F(X)
# 残差连接：输出 = F(X) + X

# 为什么？防止梯度消失，让深层网络能训练
```

