# 计算中循环的本质和来源

## 计算的两种循环

### 1. 时间循环：顺序执行的真正循环

```python
# 这是你在代码中看到的显式循环
def run_transformer(input_tensor):
    output = input_tensor
    for i in range(num_layers):  # 真正的for循环
        output = transformer_layer_i(output)  # 第i层
    return output
```

***编译器看到的数据流：***

```
时间步1: 输入 → Layer1计算 → 中间结果1
时间步2: 中间结果1 → Layer2计算 → 中间结果2  
时间步3: 中间结果2 → Layer3计算 → 中间结果3
...
时间步N: 中间结果(N-1) → LayerN计算 → 最终输出
```

***关键特点：***

- 每个时间步**必须等待前一个完成**
- 这是**串行**的，但有优化空间（流水线）
- 在CPU/GPU上，这对应着**顺序执行指令**

### 2. 空间循环：编译器的"虚拟"循环

```python
# 向量加法：C[i] = A[i] + B[i]
A = [1, 2, 3, 4]
B = [5, 6, 7, 8]

# 编程语言层面：一次性操作
C = A + B  # [6, 8, 10, 12]

# 但在硬件层面，实际发生的是：
for i in range(4):  # 编译器生成的循环
    C[i] = A[i] + B[i]
```
> 这就是"空间循环"：看似一次操作，实际是重复执行相同操作，每次处理不同数据。

## 从代码到硬件：循环的层层展开

让我们跟踪一个矩阵乘法在编译器和硬件中的旅程：

### 第一步：高层代码

```python
# Python/NumPy级别
C = np.dot(A, B)  # A: [M, K], B: [K, N], C: [M, N]
```
> 这里没有显式循环，但编译器知道要做什么。

### 第二步：编译器生成循环

```cpp
// 编译器中间表示（IR）
for (int i = 0; i < M; i++) {           // 外层循环：i轴
    for (int j = 0; j < N; j++) {       // 中层循环：j轴
        float sum = 0;
        for (int k = 0; k < K; k++) {   // 内层循环：k轴
            sum += A[i][k] * B[k][j];
        }
        C[i][j] = sum;
    }
}
```

### 第三步：编译器优化循环

```cpp
// 优化1：循环分块（tiling） - 提高缓存利用率
for (int i_outer = 0; i_outer < M; i_outer += tile_size) {
    for (int j_outer = 0; j_outer < N; j_outer += tile_size) {
        for (int k_outer = 0; k_outer < K; k_outer += tile_size) {
            // 处理一个小块
            for (int i_inner = i_outer; i_inner < min(i_outer+tile_size, M); i_inner++) {
                for (int j_inner = j_outer; j_inner < min(j_outer+tile_size, N); j_inner++) {
                    float sum = 0;
                    for (int k_inner = k_outer; k_inner < min(k_outer+tile_size, K); k_inner++) {
                        sum += A[i_inner][k_inner] * B[k_inner][j_inner];
                    }
                    C[i_inner][j_inner] += sum;  // 累加到最终结果
                }
            }
        }
    }
}
```

- 外层以tile_size为步长，从左矩阵左维度为第一子层，右矩阵右维度为第二子层，共同维度为第三子层
- 内层进行tile_size内的遍历，这里可以流水以及高速缓存

### 第四步：硬件并行执行

在CPU上，SIMD指令同时处理多个数据：

在内层时，依旧可以多个并行执行

```cpp
// SIMD向量化：一次处理4个float（AVX2）
for (int i = 0; i < M; i++) {
    for (int j = 0; j < N; j += 4) {  // 每次处理4列
        __m128 sum_vec = _mm_setzero_ps();  // 4个float的向量寄存器
        
        for (int k = 0; k < K; k++) {
            // 加载A[i][k]（标量）并广播到4个位置
            __m128 a_vec = _mm_set1_ps(A[i][k]);
            
            // 加载B[k][j:j+3]（4个连续的float）
            __m128 b_vec = _mm_loadu_ps(&B[k][j]);
            
            // 向量乘加：sum_vec += a_vec * b_vec
            sum_vec = _mm_fmadd_ps(a_vec, b_vec, sum_vec);
        }
        
        // 存储4个结果
        _mm_storeu_ps(&C[i][j], sum_vec);
    }
}
```

> 这个需要根据实际的`tile_size`和`warp_size`进行决定

## 以Transformer为例

### 场景一：多头注意力中的Q@K^T（轴与并行）

```
# 高层代码
attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(d_k)
# Q: [batch, heads, seq_len, d_k]
# K: [batch, heads, seq_len, d_k]
# scores: [batch, heads, seq_len, seq_len]
```

***编译器看到的循环展开***

```cpp
// 编译器生成的6层嵌套循环
for (int b = 0; b < batch_size; b++) {            // 轴1: batch轴
    for (int h = 0; h < num_heads; h++) {         // 轴2: head轴
        for (int i = 0; i < seq_len; i++) {       // 轴3: query序列轴
            for (int j = 0; j < seq_len; j++) {   // 轴4: key序列轴
                float sum = 0;
                for (int k = 0; k < d_k; k++) {   // 轴5: 特征轴（最内层）
                    sum += Q[b][h][i][k] * K[b][h][j][k];
                }
                scores[b][h][i][j] = sum / sqrt(d_k);
            }
        }
    }
}
```

***轴的分析：每个轴的意义和优化策略***

| 轴         | 循环变量 | 数据访问模式                     | 优化策略                     |
|------------|----------|----------------------------------|------------------------------|
| Batch轴    | b        | 完全独立，无数据依赖             | 数据并行：分配给不同GPU/核心 |
| Head轴     | h        | 独立，但共享Q/K张量              | SIMD并行：多个头同时计算     |
| Query轴    | i        | 每行独立，但需要所有Key          | 循环分块：缓存友好访问       |
| Key轴      | j        | 每列独立                         | 向量化：同时计算多个Key      |
| Feature轴  | k        | 内存连续，点积计算               | 累加寄存器：减少内存访问     |

***关键洞察：轴定义了并行度***

```
一个计算任务 = 在N维空间中的点
每个轴 = 一个可以并行的维度

例子：Q@K^T计算
- 共有 batch × heads × seq_len × seq_len × d_k 次标量乘加
- 其中 batch, heads, seq_len(部分) 可以并行
- d_k 维度需要串行累加，但可以用向量化加速
```

### 场景二：LayerNorm的循环优化（循环的融合与分裂）

***LayerNorm计算：***

```
对每个位置i：y[i] = γ * (x[i] - mean) / std + β
需要先计算mean和std
```

***原始实现（两次遍历）***

```cpp
// 第一次遍历：计算均值和方差
float sum = 0, sum_sq = 0;
for (int i = 0; i < n; i++) {
    sum += x[i];
    sum_sq += x[i] * x[i];
}
float mean = sum / n;
float std = sqrt(sum_sq / n - mean * mean + eps);

// 第二次遍历：归一化
for (int i = 0; i < n; i++) {
    y[i] = gamma * (x[i] - mean) / std + beta;
}
```
> **问题**：需要遍历数据两次，内存带宽利用率低。

***优化实现（Welford算法，单次遍历）***

```cpp
float mean = 0, m2 = 0;  // m2: 二阶中心矩
for (int i = 0; i < n; i++) {
    float delta = x[i] - mean;
    mean += delta / (i + 1);
    m2 += delta * (x[i] - mean);
}
float std = sqrt(m2 / n + eps);

// 第二次遍历无法避免，但可以融合到后续操作中
```

***编译器优化：与前后操作融合***

```cpp
// 融合：LayerNorm + 线性投影
for (int i = 0; i < n; i++) {
    // LayerNorm
    float normalized = (x[i] - mean) / std;
    float y = gamma * normalized + beta;
    
    // 立即进行线性投影（不写回内存）
    float q = 0;
    for (int j = 0; j < d_model; j++) {
        q += y * W_q[j];  // 实际上需要更多循环
    }
    // 存储结果
}
```

***通过融合，避免了中间结果y的写回和读取。***

### 场景三：Q@K^T（循环的调度顺序）

***问题：6层循环，如何安排？***

```
for b in batch:
    for h in heads:
        for i in seq_q:
            for j in seq_k:
                for k in d_k:
                    compute...
```

***编译器决策过程***

```cpp
// -------- 1. 分析数据局部性 -------- //

// 访问模式分析：
Q[b][h][i][k] - 当i变化时，k连续访问 ✓ 缓存友好
K[b][h][j][k] - 当j变化时，k连续访问 ✓ 缓存友好
scores[b][h][i][j] - 当j变化时连续 ✓

// 编译器结论：最内层循环应该是k轴（连续访问）

// -------- 2. 分析并行度 -------- //

// 并行度分析：
batch轴: 完全独立，可并行度 = batch_size (通常32-128)
head轴: 完全独立，可并行度 = num_heads (通常8-16)  
seq_q轴: 行独立，可并行度 = seq_len (通常512)
seq_k轴: 列独立，但依赖所有k，可并行度 = seq_len

// 编译器决策：
// - 外层循环：batch, heads (粗粒度并行，跨GPU/多核)
// - 中层循环：seq_q, seq_k (中粒度并行，线程级)
// - 内层循环：d_k (向量化，SIMD级)

// -------- 3. GPU上的实际映射 -------- //

// CUDA线程层次映射：
blockIdx.x = batch * heads  // 一个block处理一个(batch,head)
threadIdx.y = seq_q / tile_size  // 每个线程处理tile中的一行
threadIdx.x = seq_k / tile_size  // 每个线程处理tile中的一列

// 在block内：
for (int k_tile = 0; k_tile < d_k; k_tile += tile_k) {
    // 1. 协作加载Q的tile到共享内存
    // 2. 协作加载K的tile到共享内存
    // 3. 计算tile内的局部点积
    // 4. 累加到线程局部寄存器
}

// 最后：所有线程协作规约，得到最终分数
```

# 循环优化

## 1. 循环展开（Loop Unrolling）

```cpp
// 未展开
for (int i = 0; i < 100; i++) {
    result += array[i];
}
// 展开4倍
for (int i = 0; i < 100; i += 4) {
    result += array[i];
    result += array[i+1];
    result += array[i+2];
    result += array[i+3];
}
```
为什么有用：
- 减少循环判断的次数（从100次降到25次）
- 给编译器更多优化机会（可以一次处理多个数据）
- 但会使代码变大
## 2. 循环融合（Loop Fusion）
```cpp
// 分开的循环
for (int i = 0; i < N; i++) {
    C[i] = A[i] + B[i];  // 第一次循环
}
for (int i = 0; i < N; i++) {
    D[i] = C[i] * 2;     // 第二次循环
}

// 融合后的循环
for (int i = 0; i < N; i++) {
    C[i] = A[i] + B[i];
    D[i] = C[i] * 2;     // C[i]还在缓存里，直接使用！
}
```
为什么有用：
- 减少循环开销（只循环一次）
- 提高数据局部性：C[i]计算出来后马上用，还在缓存里
- 这是AI编译器中最重要的优化之一！
## 3. 循环分裂/分布（Loop Fission/Distribution）
```cpp
// 一个复杂循环
for (int i = 0; i < N; i++) {
    A[i] = B[i] + C[i];   // 需要很多寄存器！！！
    D[i] = E[i] * F[i];   // 需要更多寄存器！！！
    G[i] = A[i] + D[i];
}

// 分裂成两个循环
for (int i = 0; i < N; i++) {
    A[i] = B[i] + C[i];
    D[i] = E[i] * F[i];
}
for (int i = 0; i < N; i++) {
    G[i] = A[i] + D[i];
}
```
为什么有用：
- 减少寄存器压力（每个循环用更少的寄存器）
- 可能提高并行性（两个循环可以同时跑在不同核心上）
- 但可能降低数据局部性
## 4. 循环交换（Loop Interchange）
```cpp
// 不好的顺序：按列访问（内存不连续）
for (int j = 0; j < 1000; j++) {      // 列循环在外
    for (int i = 0; i < 1000; i++) {  // 行循环在内
        A[i][j] = B[i][j] + C[i][j];  // 跳跃访问内存！
    }
}

// 好的顺序：按行访问（内存连续）
for (int i = 0; i < 1000; i++) {      // 行循环在外
    for (int j = 0; j < 1000; j++) {  // 列循环在内
        A[i][j] = B[i][j] + C[i][j];  // 连续访问内存！
    }
}
```
为什么有用：
- 减少寄存器压力（每个循环用更少的寄存器）
- 可能提高并行性（两个循环可以同时跑在不同核心上）
- 但可能降低数据局部性
## 5. 循环平铺（Loop Tiling）
```cpp
// 原始的大循环
for (int i = 0; i < 1024; i++) {
    for (int j = 0; j < 1024; j++) {
        // 访问A[i][j], B[i][j]...
    }
}

// 平铺后（块大小=64）
for (int ii = 0; ii < 1024; ii += 64) {   // 外层：块循环
    for (int jj = 0; jj < 1024; jj += 64) {
        // 处理一个64×64的小块
        for (int i = ii; i < ii+64; i++) {      // 内层：块内循环
            for (int j = jj; j < jj+64; j++) {
                // 现在数据量小，可以放在缓存里
            }
        }
    }
}
```
为什么有用：
- 适配缓存大小：小块数据可以完全放在快速缓存中
- 适配NPU的片上内存：NPU的SRAM很小，必须分块
- 这是AI编译器的核心优化！
### 6. 循环分块（Loop Blocking）
和平铺基本是同一概念，有些人会区分：
- 平铺（Tiling）：更通用
- 分块（Blocking）：特指为了适配内存层次的分块
### 7. 循环向量化（Loop Vectorization）
```cpp
// 标量计算（一次一个）
for (int i = 0; i < N; i++) {
    C[i] = A[i] + B[i];
}

// 向量化计算（假设一次处理4个）
for (int i = 0; i < N; i += 4) {
    // 一条指令同时加4个数
    vector4 va = load_vector(&A[i]);
    vector4 vb = load_vector(&B[i]);
    vector4 vc = vector_add(va, vb);
    store_vector(&C[i], vc);
}
```
为什么有用：
- 现代CPU/GPU/NPU都有向量指令（SIMD）
- 一次处理4/8/16/32个数据，性能提升显著
## 8. 循环并行化（Loop Parallelization）
```cpp
// 串行循环
for (int i = 0; i < N; i++) {
    C[i] = A[i] + B[i];  // 顺序执行
}

// 并行循环（OpenMP示例）
#pragma omp parallel for
for (int i = 0; i < N; i++) {
    C[i] = A[i] + B[i];  // 多个线程同时执行
}
```
为什么有用：
- 利用多核CPU/GPU/NPU的并行计算能力
- NPU可能有成百上千个小计算单元，必须并行

# 循环映射


