# NPU内存映射I/O（MMIO）

## 1. 地址空间模型

### 1.1 地址空间定义

该 NPU 采用分段式地址空间 (Segmented Address Space) 设计：

```
地址结构: [tag:2bit][offset:37bit][reserved:25bit]

地址空间布局:
├─ 0x0000000000 ~ 0x1FFFFFFFFF  (128GB) Physical Address Space
├─ 0x2000000000 ~ 0x3FFFFFFFFF  (128GB) Memory Port Space (Cacheable)
└─ 0x4000000000 ~ 0x5FFFFFFFFF  (128GB) System Port Space (Non-cacheable)
```

### 1.2 地址转换函数

定义地址转换映射 `f: V → P`：

```cpp
// 基本变换
PHY_ADDR(v) = v & 0x1FFFFFFFFFUL              // 提取物理偏移
MEM_ADDR(v) = PHY_ADDR(v) ∨ 0x2000000000UL   // 映射到内存端口
SYS_ADDR(v) = PHY_ADDR(v) ∨ 0x4000000000UL   // 映射到系统端口

// 逆向变换
ADDR_OFFSET(v) = v & 0x1FFFFFFFFFUL
PORT_ID(v) = (v >> 37) & 0x3
```

## 2. 访问语义 (Access Semantics)

### 2.1 端口特性

| 端口类型 | 地址前缀 | Cache | 一致性模型 | 访问延迟 | 应用场景 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| Memory Port | 0x2xxx | Enable | Relaxed | Low | 数据计算、批量传输 |
| System Port | 0x4xxx | Bypass | Sequential | High | MMIO、同步原语 |

### 2.2 内存一致性模型

***Memory Port: ***
  - Load/Store 经过 L1/L2 Cache
  - 遵循 Release Consistency
  - 需要显式同步操作保证可见性

***System Port:***
  - Load/Store 直达物理设备
  - 遵循 Sequential Consistency  
  - 天然保证跨核可见性

## 3. 存储层次结构 (Memory Hierarchy)

### 3.1 片上存储器拓扑

```
NPU Chip (16 CCPU + 4 HCPU + 64 Tiles)
│
├─ L3 DDR Memory
│  └─ Base: MEM_ADDR(0x1000000000), Size: 64GB
│
├─ L2 System Buffer  
│  └─ Base: 0x2FF6000000, Size: 12MB
│
├─ L1.5 HCPU Memory
│  └─ Base: MEM_ADDR(0x88000000), Size: 32MB
│
└─ L1 Tile Memory (64 Tiles × 15 Clusters)
   ├─ Global Tile UM: MEM_ADDR(0x90000000) + tile_id × 2MB
   ├─ Local Cluster UM: MEM_ADDR(0x80000000) + (tile_id & 0x3) × 2MB
   ├─ Cluster SRAM: MEM_ADDR(0x0), Size: 256KB (low latency)
   └─ Interleaved UM: MEM_ADDR(0x40000), Size: 7.75MB
```

### 3.2 地址计算公式

```cpp
// Tile 寻址（全局视图）
TileGlobalAddr(tile_id, offset) = 0x2090000000 + tile_id × 0x200000 + offset
    where: tile_id ∈ [0, 63], offset ∈ [0, 0x1FFFFF]

// Tile 寻址（集群本地视图）
TileLocalAddr(tile_id, offset) = 0x2080000000 + (tile_id mod 4) × 0x200000 + offset
    where: tile_id mod 4 为集群内 tile 索引
```

## 4. 硬件实现机制

### 4.1 地址解码逻辑

```
CPU 发出访问地址 A
    ↓
[Tag 解码器] 检查 A[38:37]
    ↓
    ├─ Tag = 00 → 物理地址（未使用）
    ├─ Tag = 01 → Memory Port
    │   ├─ 查询 L1 Cache
    │   ├─ 查询 L2 Cache
    │   └─ 若 miss → 访问物理地址 A[36:0]
    │
    └─ Tag = 10 → System Port
        └─ 直接访问物理地址 A[36:0]，bypass cache
```

### 4.2 Cache 一致性协议

***Memory Port 访问:***
- Write → Write-Back (延迟写入物理内存)
- Read  → Cache Line Fill (可能读到脏数据)

***System Port 访问:***
- Write → Write-Through (立即写入物理内存)
- Read  → Direct Access (总是最新数据)

## 5. 编程模型 (Programming Model)

### 5.1 典型访问模式

***Pattern 1: 计算密集型负载***

```cpp
// 使用 Memory Port 获得缓存加速
volatile float *A = (float *)MEM_ADDR(DDR_BASE);
volatile float *B = (float *)MEM_ADDR(DDR_BASE + 0x10000);

for (int i = 0; i < N; i++) {
    B[i] = A[i] * 2.0f;  // 缓存友好访问
}
```

***Pattern 2: MMIO 设备控制***

```cpp
// 使用 System Port 保证写可见性
#define DMA_CTRL_REG  0xFF000000
#define DMA_STATUS    0xFF000004

*(volatile uint32_t *)SYS_ADDR(DMA_CTRL_REG) = START_DMA;

while (!(*(volatile uint32_t *)SYS_ADDR(DMA_STATUS) & DMA_DONE)) {
    // Busy wait，必须用 SYS_ADDR 避免读缓存旧值
}
```

***Pattern 3: 多核同步***

```cpp
// Producer (Core 0)
*(uint32_t *)MEM_ADDR(shared_data) = 0xDEADBEEF;
__sync_synchronize();  // Memory fence
*(volatile uint32_t *)SYS_ADDR(flag_addr) = 1;  // Release

// Consumer (Core 1)
while (!(*(volatile uint32_t *)SYS_ADDR(flag_addr))) {}  // Acquire
__sync_synchronize();
uint32_t data = *(uint32_t *)MEM_ADDR(shared_data);
```

## 6. 关键宏定义语义
### 6.1 缓存控制宏

```cpp
// 判断地址是否可缓存
IS_CACHEABLE_ADDR(addr) ⟺ 
    addr ∈ [DDR ∪ HMEM ∪ TILE_UM ∪ CLUSTER_UM ∪ SRAM ∪ SYSBUF]

// 强制非缓存访问
NON_CACHEABLE_ADDR(addr) = SYS_ADDR(addr)
NON_CACHEABLE_VAR(addr)  = *(typeof(addr))(SYS_ADDR(addr))
```

### 6.2 特殊通信通道

```cpp
// 主机通信（tohost 机制）
#define TOHOST_NONCACHEABLE_ADDR 0x577FFFFFF0UL
#define SET_TOHOST(val) *(uint64_t *)(TOHOST_NONCACHEABLE_ADDR) = (val)

// 用途：NPU → 主机 CPU 的单向通知机制
```

## 7. 性能特性分析

### 7.1 访问延迟模型 (Cycles)

```
L1 SRAM (Memory Port):      ~10 cycles
L1.5 Tile UM (Memory Port):  ~20 cycles
L2 SYSBUF (Memory Port):     ~50 cycles
L3 DDR (Memory Port):        ~100 cycles

同一地址 via System Port:   +50% latency penalty
                            (cache bypass overhead)
```

### 7.2 带宽特性

***Memory Port:***
- Sequential Access: 接近理论带宽
- Random Access:     缓存命中率影响大

***System Port:***
  - Sequential Access: ~70% 理论带宽
  - Random Access:     性能稳定，不受缓存影响

## 8. 设计权衡 (Design Tradeoffs)

### 8.1 优势

1. 灵活性: 同一物理地址支持两种访问语义
2. 性能: 缓存敏感数据获得加速
3. 正确性: 硬件控制路径保证一致性
4. 简化设计: 地址映射在 CPU 侧完成，互连网络统一

### 8.2 代价
1. 地址空间浪费: 每 1GB 物理内存消耗 3GB 虚拟地址空间
2. 编程复杂度: 开发者需理解两种端口语义
3. 调试难度: 地址错误（MEM vs SYS）导致隐晦 bug

## 9. 与经典架构对比

| 特性 | 该 NPU | x86_64 + GPU | ARM + Mali GPU |
| :--- | :--- | :--- | :--- |
| 地址映射 | 显式端口前缀 | PCIe BAR + IOMMU | Device Tree MMIO |
| 缓存控制 | 地址 tag 编码 | PAT/MTRR 页属性 | Memory Type 字段 |
| 一致性 | 端口级分离 | MESI 协议 | ACE 总线协议 |
| 片上内存 | Tile UM 分层 | Shared Memory 隐式 | GPU Local Memory |

## 10. 安全性考虑

### 10.1 潜在漏洞

```cpp
// 错误示例：缓存不一致导致 TOCTOU
if (*(uint32_t *)MEM_ADDR(auth_flag) == 1) {  // Check (缓存值)
    // 攻击者通过 DMA 修改 auth_flag
    do_privileged_op();  // Use (实际值已变)
}

// 正确示例：使用 System Port
if (*(volatile uint32_t *)SYS_ADDR(auth_flag) == 1) {
    do_privileged_op();
}
```

### 10.2 访问控制

```cpp
// 地址范围检查
#define IS_DDR_ADDR(addr) (DDR_BASE <= (addr) && (addr) < DDR_BASE + DDR_SIZE)
#define IS_VALID_TILE(id) ((id) < TILE_COUNT)

// 防止越界访问
if (!IS_VALID_TILE(tile_id)) return ERROR_INVALID_TILE;
```

## 11. 总结：核心抽象

该 MMIO 机制本质上实现了：

```
Address Space Virtualization + Memory Consistency Control
```

- 通过地址空间划分，将硬件特性（缓存/非缓存）编码为地址属性，
- 使软件能以统一的 load/store 指令访问异构存储层次，
- 同时保留对访问语义的细粒度控制。

***形式化表示***

```
∀ physical_addr p ∈ [0, 0x1FFFFFFFFF]:
  ∃ cached_view   = p | 0x2000000000  (fast, relaxed)
  ∃ uncached_view = p | 0x4000000000  (slow, strict)

where:
  read(cached_view)   → may_return_stale_data()
  read(uncached_view) → always_return_latest()
```