
# 体系架构

总共分为五层：

## 第0层：物理硬件层
主要包括计算硬件、内存体系、以及互联网络等等

## 第1层：硬件抽象层
针对物理硬件层，会设计出一套抽象层表达，如cuda、Rocm等等

## 第2层：算子与内核层
包括了基础算子库和特化优化的手工优化内核

## 第3层：AI编译器与优化层
简述为前端优化、多级中间优化、目标硬件代码生成、自动调优

主流AI编译器：

```
TensorRT:
  优势: 
    - NVIDIA官方，与CUDA生态深度集成
    - 支持多种精度：FP32/FP16/INT8/FP8
    - 动态形状支持（通过优化配置文件）
    - 支持插件机制
  工作流: ONNX → TensorRT Engine

TVM:
  优势:
    - 跨硬件支持（CPU/GPU/ASIC）
    - 自动调度（AutoTVM, Ansor）
    - Relay IR表示
    - 可扩展的代码生成
  工作流: PyTorch → Relay IR → 调优 → 目标代码

XLA (JAX/TensorFlow):
  优势:
    - 与框架深度集成
    - 即时编译（JIT）
    - 自动算子融合
    - 支持TPU
  工作流: TF Graph → HLO IR → 编译

OpenAI Triton:
  优势:
    - Python原生GPU编程
    - 自动内存管理和并行
    - 灵活的块状编程模型
  工作流: Triton Kernel → PTX → GPU执行

MLIR (多级中间表示):
  优势:
    - 统一的编译器基础设施
    - 方言系统（LLVM/HLO/Linalg等）
    - 渐进式 lowering
  工作流: 框架IR → MLIR Dialects → 目标IR
```

量化优化子栈：

```
# 量化全流程
class QuantizationPipeline:
    def quantize_model(self, fp32_model, calibration_data):
        # 1. 训练后量化（PTQ）
        if self.method == "PTQ":
            # 校准阶段
            calibrated_model = self.calibrate(fp32_model, calibration_data)
            
            # 量化阶段
            quantized_model = self.apply_quantization(calibrated_model)
            
            # 量化感知训练（QAT）可选
            if self.use_qat:
                quantized_model = self.qat_finetune(quantized_model)
        
        # 2. 训练时量化（QAT）
        elif self.method == "QAT":
            # 插入伪量化节点
            model_with_fake_quant = self.insert_fake_quant_nodes(fp32_model)
            
            # 微调
            quantized_model = self.finetune_with_fake_quant(model_with_fake_quant)
            
            # 转换为真实量化模型
            quantized_model = self.convert_to_real_quant(quantized_model)
        
        # 3. 混合精度量化
        elif self.method == "mixed_precision":
            # 敏感度分析
            sensitivity_map = self.analyze_sensitivity(fp32_model)
            
            # 分层配置精度
            quant_config = self.mixed_precision_config(sensitivity_map)
            
            quantized_model = self.apply_mixed_precision(fp32_model, quant_config)
        
        return quantized_model
    
    def supported_precisions(self):
        return {
            "FP32": "全精度，用于敏感层",
            "FP16": "半精度，主流推理",
            "BF16": "大脑浮点，训练友好",
            "FP8": "8位浮点，Hopper支持",
            "INT8": "8位整数，广泛使用",
            "INT4": "4位整数，最近进展",
            "AWQ": "激活感知权重量化",
            "GPTQ": "GPT模型专用量化",
            "SmoothQuant": "平滑量化，解决激活异常值"
        }
```

## 第4层：推理引擎与运行时层

架构伪代码：

```cpp
// 现代推理引擎的模块化设计
class InferenceEngine {
private:
    // 核心子系统
    ResourceManager resource_mgr_;      // 资源管理
    RequestScheduler request_scheduler_; // 请求调度
    MemoryManager memory_mgr_;          // 内存管理
    KernelDispatcher kernel_dispatcher_; // 内核分发
    Profiler& profiler_;                // 性能分析
    
    // 缓存系统
    GraphCache graph_cache_;            // 计算图缓存
    KV_Cache_Manager kv_cache_mgr_;     // KV缓存管理
    WeightCache weight_cache_;          // 权重缓存
    
    // 并行执行
    StreamManager stream_mgr_;          // CUDA流管理
    PipelineExecutor pipeline_executor_; // 流水线执行器
    
public:
    // 初始化引擎
    bool initialize(const EngineConfig& config) {
        // 1. 加载优化后的计算图
        load_optimized_graph(config.model_path);
        
        // 2. 初始化内存管理器
        memory_mgr_.initialize(config.memory_pool_size);
        
        // 3. 预热：预分配资源，编译kernel
        warmup(config.warmup_batches);
        
        // 4. 启动监控线程
        start_monitoring_thread();
        
        return true;
    }
    
    // 处理请求的核心流程
    InferenceResult process_request(const Request& req) {
        // 阶段A：请求预处理（TrafficLight决策点）
        RequestBatch batch;
        if (!request_scheduler_.should_add_to_batch(req, batch)) {
            // 放入等待队列
            request_scheduler_.enqueue(req);
            return wait_for_result(req.id);
        }
        
        // 阶段B：计算图执行准备
        ExecutionContext ctx = prepare_execution(batch);
        
        // 阶段C：多阶段流水线执行
        PipelineResult result = pipeline_executor_.execute(ctx);
        
        // 阶段D：结果后处理与返回（Parking实现）
        return postprocess_and_return(result);
    }
};
```

包括了TrafficLight调度器、PagedAttention内存管理、Parking实现等等

主流如vLLM、TensorRT-LLM等等

## 第5层：服务化与API层

## 第6层：编排与部署层

## 第7层：应用与服务层