# QDQ

## 背景：量化的问题

### 问题 1：直接量化的损失

```
原始权重（FP32）：  0.12345
简单量化（INT8）：  0（误差巨大！）

如果你不在训练时考虑量化误差，
推理时的精度会严重下降。
```

### 问题 2：训练时和推理时不一致

```
训练：用FP32权重
      精度：99%
      
推理：用INT8权重
      精度：70%（严重掉点）
```

## 目的

在训练时模拟推理中的量化过程。

## 具体流程

```
原始权重
  ↓
[Q] QuantizeLinear：FP32 → INT8（量化）
  ↓
[DQ] DequantizeLinear：INT8 → FP32（反量化）
  ↓
得到近似权重（带量化误差）
  ↓
进行前向传播
  ↓
计算梯度（梯度"看到了"量化误差）
  ↓
反向传播，更新权重
  ↓
权重适应了量化误差
```

## 为什么要 Q 再 DQ？

### 如果只有 Q（只量化不反量化）

```
权重：FP32 0.12345 → INT8 0 → 直接计算
                     ↑
              精度丧失，无法训练
```

**问题**：INT8 离散，无法反向传播梯度。

### `Q + DQ`（先量化再反量化）

```
权重：FP32 0.12345 
           ↓ [Q]
           INT8 0
           ↓ [DQ]  
           FP32 0（失去小数部分）
           ↓
        梯度能流动！
```

***优势：***

- Q 和 DQ 都是可微的
- 梯度能反向传播
- 权重能根据量化误差调整

## 量化感知训练（QAT）

```bash
# 训练时（使用QDQ）
weight_original = 0.12345
weight_q = Quantize(weight_original)  # → INT8: 0
weight_dq = Dequantize(weight_q)      # → FP32: 0（失精度）
output = model(weight_dq)  # 精度：95%（考虑到量化误差）

# 计算梯度时，权重会学会"容错"量化误差
# 反向传播：权重会自动调整

# 推理时（直接用INT8）
weight = 0  # INT8
output = model(weight)  # 精度：94%  ✓ 只掉1%
```

***为什么ONNX这样做？***

- ✅ 在训练时让梯度"看到"量化误差
- ✅ 微调权重以适应量化
- ✅ 推理时移除DQ，使用INT8硬件加速

# 编译器视角

```mlir
// ONNX导出的QAT模型
%w_q = QuantizeLinear(%w_fp32, scale, zero_point)
%w_dq = DequantizeLinear(%w_q, scale, zero_point)
%out = MatMul(%activation, %w_dq)

// 编译器的三个pass：

// Pass 11: convertQDQGraph
// 识别这个QDQ pattern，理解量化参数

// Pass 12: quant  
// 把DQ删除，直接用INT8计算
%w_q_i8 = QuantizeLinear(...)
%out = MatMul(%activation, %w_q_i8)  // ← 直接用INT8

// Pass 13: fold-constant
// 把QuantizeLinear折叠成常数
%w_const_i8 = Constant<INT8>[...]  // ← 预先量化
%out = MatMul(%activation, %w_const_i8)
```