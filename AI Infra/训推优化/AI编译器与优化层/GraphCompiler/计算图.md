# 分析计算图

以这个流程为例：

```
输入张量 X: [B, S, D] (Batch, Sequence, Hidden)
│
├─── 子图A：多头自注意力 (MHSA) ────────────── 核心计算密集型区域
│   │
│   ├─ 1. 层归一化 (Pre-Norm)
│   │   └→ X_norm: [B, S, D]                      // 逐点操作，访存密集
│   │
│   ├─ 2. 投影与重排 (Q/K/V Projection & Split)
│   │   ├→ Q = Linear(X_norm, W_q): [B, S, D]
│   │   ├→ K = Linear(X_norm, W_k): [B, S, D]
│   │   ├→ V = Linear(X_norm, W_v): [B, S, D]    // 三个独立GEMM，计算密集
│   │   │
│   │   ├→ Q_reshaped: [B, S, H, Dh]→[B, H, S, Dh] // H:头数，Dh=D/H
│   │   ├→ K_reshaped: [B, S, H, Dh]→[B, H, S, Dh] // 数据重排，引入转置
│   │   └→ V_reshaped: [B, S, H, Dh]→[B, H, S, Dh] // 内存不连续访问
│   │
│   ├─ 3. 注意力核心计算
│   │   ├→ S = Q @ K^T / sqrt(Dh): [B, H, S, S]   // 大GEMM，O(S²)复杂度
│   │   ├→ P = Softmax(S, dim=-1): [B, H, S, S]   // 逐点规约，访存密集
│   │   └→ O_heads = P @ V: [B, H, S, Dh]         // 另一个大GEMM
│   │
│   ├─ 4. 合并与输出投影
│   │   ├→ O_concat: [B, H, S, Dh]→[B, S, H, Dh]→[B, S, D] // 反向重排+拼接
│   │   └→ Attn_Out = Linear(O_concat, W_o): [B, S, D] // 最终GEMM
│   │
│   └─ 5. 残差连接
│       └→ Z = X + Attn_Out: [B, S, D]            // 逐点加法，访存密集
│
├─── 子图B：前馈网络 (FFN) ────────────────────── 另一核心计算区
│   │
│   ├─ 1. 层归一化 (Pre-Norm)
│   │   └→ Z_norm: [B, S, D]                      // 逐点操作
│   │
│   ├─ 2. 两层全连接层
│   │   ├→ FF1 = GELU(Linear(Z_norm, W1)): [B, S, 4D] // GEMM + 逐点激活
│   │   └→ FF2 = Linear(FF1, W2): [B, S, D]       // GEMM
│   │
│   └─ 3. 残差连接
│       └→ Layer_Out = Z + FF2: [B, S, D]         // 逐点加法
│
└─── 输出: [B, S, D] (作为下一层输入)
```

编译器会分析这个图，识别出几类特征迥异的算子，并为每类设计不同的优化策略：

## 计算密集型算子

### 特点

**算术运算远多于内存访问**，性能受限于计算单元吞吐量。

### 主要内容

**GEMM**：图中绝大部分计算量所在。

Q/K/V投影、Q@K^T、P@V、输出投影、FFN中的两个全连接。

### 优化焦点

循环分块、数据布局、利用专用硬件（如GPU的Tensor Core）。

## 访问密集型算子

### 特点

**内存访问远多于算术运算**，性能受限于内存带宽。

### 主要内容

逐点操作：LayerNorm、GELU/ReLU、Softmax、Dropout、残差Add。

### 优化焦点

**算子融合**。将它们合并到相邻的GEMM内核中，避免中间结果写回全局内存。

## 数据重排与形状变换算子

### 特点

不进行实质计算，只改变数据在内存中的排列。

### 主要内容

- **Reshape/Transpose**：多头分割与合并时的 [B, S, H, Dh] <-> [B, H, S, Dh]。
- **编译器视角**：这是性能陷阱。显式的转置会导致内存拷贝，破坏数据局部性。

### 优化焦点

通过**隐式重排**消除它。例如，让GEMM内核直接按 `[B, H, S, Dh]` 的逻辑布局读取 `[B, S, D]` 的物理内存。

# 计算图运行本质



# 核心优化策略

## 策略1：算子融合

这是最重要的优化，目的是减少对高带宽内存的访问。

### 横向融合：将多个逐点算子合并。

```c
// 未融合：三次读/写全局内存
output = gelu(input);
output = dropout(output);
output = output + residual;

// 融合后：一次读，一次写，中间在寄存器/高速缓存完成
fused_gelu_dropout_add(input, residual, output);
```

### 纵向融合（GEMM Epilogue Fusion）：将逐点算子作为“尾声”融合进GEMM。

```c
// 未融合：GEMM写回内存，LayerNorm再从内存读
C = gemm(A, B); // 写回内存
output = layernorm(C); // 从内存读

// 融合：GEMM结果不写回，直接在片上缓存进行LayerNorm计算
output = gemm_with_layernorm_epilogue(A, B);
```

现代编译器（如TVM、XLA） 可以将 GEMM + BiasAdd + GELU + ResidualAdd 自动融合为一个宏内核。

## 策略2：内存规划与原地计算

- **内存分配器**：识别张量生命周期，让 `Attn_Out` 和后续计算复用 X 的内存（如果 X 已不再需要）。
- **原地计算**：`Residual Add` 可以原地进行， `Z = X + Attn_Out` 的结果可以覆盖 Z 或 X 的存储空间。

## 策略3：计算重排与常数折叠

- **提前计算**：固定的位置编码矩阵可以在编译期计算好，作为常量嵌入。
- **公共子表达式消除**：在同一层中，X_norm 被用于计算 Q, K, V，应确保只计算一次。
- **死代码消除**：推理时去掉 Dropout 分支。

## 策略4：针对注意力的特殊优化

由于 `Q@K^T` 产生 `[B, H, S, S]` 中间矩阵（巨大），催生了专用优化：

- **Flash Attention**：通过平铺和在SRAM/缓存中进行Softmax重计算，避免将庞大的注意力矩阵写回HBM。从计算图上看，它融合了整个注意力路径（MatMul, Scale, Mask, Softmax, MatMul）。
- **内存高效的注意力**：编译器可以自动应用类似Flash Attention的切分和融合策略。