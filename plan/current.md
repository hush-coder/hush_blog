AI编译器与高性能计算专家学习路径（从零到精通）

一、计算机系统基础（第1-2个月）

意义：建立程序在硬件上执行的根本直觉

1.1 C语言与汇编（3周）

· 学习内容：
  · 《C Programming Language》前8章
  · x86-64汇编基础：寄存器、指令、栈帧
· 实践项目：
  · 实现内存分配器（malloc/free）
  · 编写函数，反汇编分析其栈帧
· 检验标准：
  · 能解释C中指针和数组的区别
  · 能阅读简单的x86汇编代码

1.2 计算机组成与内存层次（3周）

· 学习内容：
  · 《Computer Systems: A Programmer's Perspective》第5-6章
  · 缓存工作原理：缓存行、关联度、命中/失效
· 实践项目：
  · 编写矩阵乘法，比较不同遍历顺序的性能差异
  · 用perf stat测量缓存命中率
· 检验标准：
  · 能解释为什么for(i) for(j) a[i][j]比for(j) for(i) a[i][j]快
  · 能计算给定访问模式的缓存命中率

1.3 并行计算基础（2周）

· 学习内容：
  · SIMD指令集（SSE/AVX）
  · 多线程基础：OpenMP
· 实践项目：
  · 用AVX指令优化向量点积
  · 用OpenMP并行化矩阵乘法
· 检验标准：
  · 能编写使用SIMD内禀函数的代码
  · 能识别数据竞争和false sharing

二、编译原理核心（第3-4个月）

意义：理解代码自动优化的理论基础

2.1 中间表示与数据流（4周）

· 学习内容：
  · LLVM IR：基本语法、控制流、SSA形式
  · 数据流分析：到达定义、活跃变量
· 实践项目：
  · 编写C代码，用clang -S -emit-llvm生成IR
  · 实现简单的活跃变量分析算法
· 检验标准：
  · 能手动将小型C函数翻译为LLVM IR
  · 能解释SSA中φ节点的作用

2.2 经典优化技术（4周）

· 学习内容：
  · 局部优化：常量传播、死代码消除
  · 循环优化：循环不变代码外提、归纳变量
· 实践项目：
  · 在LLVM中编写简单Pass实现上述优化
  · 分析优化前后的IR变化
· 检验标准：
  · 能识别给定代码片段的可优化点
  · 能解释每种优化的安全条件

三、GPU体系结构与CUDA（第5-6个月）

意义：掌握大规模并行计算硬件的编程模型

3.1 CUDA编程模型（4周）

· 学习内容：
  · GPU架构：SM、Warp、线程层次
  · 内存模型：全局、共享、寄存器内存
· 实践项目：
  · 实现向量加法、矩阵乘法（基础版）
  · 使用共享内存优化矩阵乘法
· 检验标准：
  · 能正确设置grid和block维度
  · 能解释bank conflict及避免方法

3.2 GPU性能优化（4周）

· 学习内容：
  · 访存优化：合并访问、缓存利用
  · 指令优化：控制流分化、指令吞吐
· 实践项目：
  · 使用Nsight Compute分析核函数瓶颈
  · 优化卷积核函数达到接近cuDNN性能
· 检验标准：
  · 能读懂Nsight Compute报告
  · 能针对特定瓶颈提出优化方案

四、MLIR编译器框架（第7-8个月）

意义：掌握现代编译器基础设施

4.1 MLIR核心概念（4周）

· 学习内容：
  · Dialect、Operation、Attribute系统
  · Pattern Rewriting机制
· 实践项目：
  · 完成MLIR官方Toy教程
  · 自定义简单Dialect并实现 lowering
· 检验标准：
  · 能解释MLIR相比LLVM的优势
  · 能实现简单的IR到IR转换

4.2 MLIR应用实践（4周）

· 学习内容：
  · 多面体编译（Polyhedral）
  · 硬件特定Dialect（GPU、Vector）
· 实践项目：
  · 使用Affine Dialect优化循环嵌套
  · 实现CPU/GPU代码生成pipeline
· 检验标准：
  · 能用MLIR表达复杂循环变换
  · 能解释从高层IR到硬件代码的完整 lowering 链条

五、AI Infra与算子优化（第9-12个月）

意义：将理论应用于AI计算的实际问题

5.1 AI计算图与编译器（4周）

· 学习内容：
  · ONNX模型格式与计算图
  · 图级优化：算子融合、常量折叠
· 实践项目：
  · 使用ONNX Runtime加载和运行模型
  · 实现简单的图优化Pass
· 检验标准：
  · 能解释常见图优化技术
  · 能手动优化给定计算图

5.2 算子优化方法论（8周）

· 系统性优化流程：
  ```
  步骤1：性能分析
    - 使用nsys timeline分析
    - 确定瓶颈类型：内存绑定/计算绑定
  
  步骤2：内存访问优化
    - 循环分块（Tiling）：匹配缓存大小
    - 内存布局转换：NHWC vs NCHW
    - 预取（Prefetching）
  
  步骤3：并行化优化
    - 线程层次设计
    - Warp级优化
    - 向量化
  
  步骤4：指令级优化
    - 使用Tensor Core
    - 循环展开
    - 双缓冲
  
  步骤5：验证与迭代
    - 性能测试
    - 精度验证
  ```
· 实践项目：
  · 从零实现高性能卷积算子
  · 实现Transformer注意力层的优化版本
· 检验标准：
  · 能系统性地优化新算子
  · 优化后性能达到硬件理论峰值50%以上

5.3 量化与稀疏化（4周）

· 学习内容：
  · 后训练量化（PTQ）与量化感知训练（QAT）
  · 稀疏存储格式与计算
· 实践项目：
  · 实现权重量化工具
  · 实现稀疏矩阵乘法
· 检验标准：
  · 能解释不同量化策略的精度/速度权衡
  · 能实现结构化稀疏的加速

六、系统集成与前沿（第13-18个月）

意义：构建完整的AI编译栈并跟踪前沿

6.1 完整编译器实现（8周）

· 项目：实现小型AI编译器
  ```
  前端：PyTorch模型 → 自定义计算图IR
  中端：图优化 → MLIR lowering
  后端：MLIR → LLVM IR → GPU代码
  ```
· 检验标准：
  · 能完整编译简单模型
  · 端到端性能可接受

6.2 前沿技术研究（长期）

· 跟踪方向：
  · 自动调度（Auto-scheduler）
  · 硬件感知神经网络架构搜索（HW-NAS）
  · 新硬件支持（Chiplet、存算一体）
· 实践：
  · 复现相关论文
  · 贡献开源项目

学习方法论

每周学习结构：

```
周一至周四（每日3小时）：
  1小时：理论学习（书籍/论文）
  1小时：代码实践
  1小时：调试与性能分析

周五（2小时）：
  本周知识总结
  撰写技术笔记

周六（3小时）：
  项目开发
  性能优化迭代

周日（1小时）：
  制定下周计划
  社区交流（GitHub/知乎/StackOverflow）
```

知识验证循环：

```
学习概念 → 编写最小实现 → 性能剖析 → 
发现问题 → 深入研究 → 优化改进 → 
形成文档 → 教授他人（验证理解）
```

必备工具链：

· 性能分析：perf, nvprof, nsys, Nsight Compute
· 编译器：LLVM/MLIR代码库
· 硬件：具备GPU的服务器
· 协作：Git、代码审查

阶段性里程碑

初级（6个月）：

· 能优化简单算子达到理论性能30%
· 能理解并修改编译器优化Pass

中级（12个月）：

· 能设计新算子的优化方案
· 能实现新的编译器变换
· 性能优化达到理论值50%

高级（18个月+）：

· 能设计领域特定编译流程
· 能针对新硬件设计优化策略
· 能指导他人进行性能优化

紧急速成方案（3个月版）

基于沐曦实习的实践驱动学习：

```
第1个月：掌握工具链与基础
  1. 精读沐曦代码库的编译pipeline
  2. 使用Nsight工具分析现有算子
  3. 复现一个简单算子的优化过程

第2个月：深入一个垂直方向
  1. 选择量化/图优化/代码生成中一个方向
  2. 阅读该方向5篇核心论文
  3. 实现一个改进版本

第3个月：构建完整叙事
  1. 整理实习项目中的技术难点
  2. 形成"问题-分析-解决"的完整故事
  3. 模拟面试训练
```

面试准备重点：

1. 基础牢固：编译原理、体系结构必须能白板推导
2. 项目深入：至少一个项目能讲30分钟技术细节
3. 系统思维：能描述从模型到硬件的完整技术栈

此路径需要至少2000小时的有效学习时间。建议保持每日3-4小时，周末加长的节奏。所有知识必须通过实践验证，仅阅读无效。